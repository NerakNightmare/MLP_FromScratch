import numpy as np
#Activation functions for output Layer
def linear(z, derivative=False):
    a = z
    if derivative:
        da = np.ones(z.shape) 
        return a, da
    return a

def logistic(z, derivative=False):
    a = 1/(1 + np.exp(-z)) 
    if derivative:
        da = np.ones(z.shape) 
        return a, da
    return a

def softmax(z, derivative=False): 
    e = np.exp(z - np.max(z, axis=0)) 
    a = e / np.sum(e, axis=0)
    if derivative:
        da = np.ones(z.shape) 
        return a, da
    return a

# Activation functions for hidden layers
def tanh(z, derivative=False):
    a = np.tanh(z)
    if derivative:
        da = (1 - a) * (1 + a) 
        return a, da
    return a

def relu(z, derivative=False):
    a = z (z >= 0)
    if derivative:
        da = np.array(z >= 0, dtype=float) 
        return a, da
    return a
def logistic_hidden (z, derivative=False): 
    a = 1/(1+ np.exp(-z))
    if derivative:
        da = a * (1 - a)
        return a, da
    return a